======================================
GBDT怎么解决时间序列中外推能力差的问题
======================================

:Date:   2022-03-21T20:57:29+08:00

[参考整理]

基于树的算法在机器学习生态系统中是众所周知的，它们主要以表格类的监督任务而闻名。在学习过程中，\ **树的分裂标准只关注相关特征和有用值的范围，梯度提升擅长学习交互**\ ，所以给定一组表格特征和要预测的目标，无需太多配置和特定的预处理就可以得到令人满意的结果。

但是\ **基于树和梯度提升模型在时间序列预测领域的表现并不好，很多人更倾向于深度学习的方法**\ 。这并不奇怪，因为\ **基于树的模型的弱点在于：在技术上无法推断出比训练数据中更高/更低的特征值**\ 。\ **他们几乎不可能预测所见区间之外的值。**

例如在，销量预测中，对于销量的\ **趋势性非常明显的城市**\ ，预测的结果非常差。

**核心的原因是tree没有办法做“外推”，真是tree系列模型本身的致命缺陷。**

.. _1-树模型为什么没有外推能力:

1 树模型为什么没有外推能力
==========================

像这样趋势性非常强的数据，GBDT是没有办法拟合的，以单Tree为例，Tree的叶子节点的值是\ **已知标签的值的平均**\ ，而对于GBDT的集成模型而言，\ **某个样本是其落在的所有子树的叶子上的值的加权平均（权重是学习率）**\ 。

这导致的问题就是我们的预测结果一定是介于\ `序列数据 <https://www.zhihu.com/search?q=%E5%BA%8F%E5%88%97%E6%95%B0%E6%8D%AE&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A311883742%7D>`__\ 的最大和最小值之间的，例如[1,2,3,4,5,6,....1000]，使用Tree或者\ `集成决策树 <https://www.zhihu.com/search?q=%E9%9B%86%E6%88%90%E5%86%B3%E7%AD%96%E6%A0%91&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A311883742%7D>`__\ 最终预测的结果在绝大部分情况下不会大于1000，这就意味着一旦我们未来的日子，销量突破新高，例如到达了2000或者3000,....等等，Tree是预测不出来的，这也是\ **为什么我使用原始标签进行预测总是发现模型预测的结果偏小**\ ，而且小了很多。

因为我使用的是所有的历史数据，我们知道电商的销量从早期的2010年到现在，已经不是一个数量级，支付宝双十一的销售额也是年年创下新高。

**这样的效应对于大型城市来说是非常明显的，北上广的电商销量基本上是往上走的就像上面这幅图一样，不过一些n线小城市的销量整体稳定，这也是为什么做bad
case的时候发现小型城市的预测比较准，大型城市的预测差的一批。**

**但是对于线性回归模型或者是NN模型来说，则没有外推能力差的问题。**

.. _2-如何解决树模型的外推能力:

2 如何解决树模型的外推能力
==========================

处理的方式就是

树模型不会外推，因此，如果要将其用于外推预测，则需要\ **创建隐含外推的特征**\ 。例如：

-  可以将简单的\ **线性模型**\ 与\ **时间变量**\ 拟合，然后\ **将预测用作特征。**

-  使用\ **残差**\ 作为\ **目标**\ 。

-  使用\ **变化率**\ 作为\ **特征**\ ，通过适当的特征工程，对树模型仍然会很有帮助，通常特征工程至关重要。

-  对标签进行变换，注意，对数变换\ **可以在一定程度消除未来和当前的差异**\ ，但是属于\ **治标不治本**\ 的方法。

-  **使用差分变换或者是增长率变换才可能消除这种差异。**

..

   但是实际上，差分变换不一定有效（我做的结果就是这样），因为差分变换之后，我们还要把差分预测的结果转化为最终的真实销量，这个时候你会发现，特么转化之后的预测销量出现负数。。。。fuck。。。这个其实很头疼，差分处理之后，模型的mse
   mae之类的确实下降了，但是销量预测为负数，业务上根本过不去，就很恶心。

下一步打算上NN模型的框架（线性回归的baseline误差太大，没法用，不过可以考虑以某种方式和GBDT集成弥补其外推能力差的问题），NN毕竟是老牌万能拟合器，趋势性的信号有希望能够拟合进模型中。

.. _3-实际应用中为什么树模型好用:

3 实际应用中为什么树模型好用
============================

**为什么很多领域的比赛数据集上的GBDT效果那么好（比神经网络）。**

回答这个问题前，先引入一个概念，目标的非线性。

非线性，应该是从原始输入到目标决策之间gap的刻画。

人的解决问题，大概就是把一个复杂目标化简解决的能力，目的是降低问题的非线性，把一个问题去解决的过程。具体到数据挖掘上，应该是把特征表达出来的能力。

具体到问题，非线性比较高的场景有，\ **序列建模，大规模离散ID建模，阴阳话识别，语音的特征表达。这些都是非线性非常高的场景，基本都超出了手动解决输入到目标之间gap的能力。**

简而言之当你的认知超越问题的难度时候，可以通过一些方法把问题去复杂化，一般GBDT的效果会比NN好。

**为什么在实际的kaggle比赛中，GBDT效果非常好？**

**Q1.kaggle上的数据和赛题有什么特点？**

**Q2.GBDT和NN有什么特点？**

**Q3.为什么你很少看见Kaggle上用SVM LR立大功？**

**Q4.怎么根据数据特点进行模型选型？**

本质上还是由\ **数据和模型决定**\ 的。

为什么kaggle里给你一种GBDT满天飞的感觉。

**Q1kaggle上的数据和赛题有什么特点？**

跟现在研究生入学，大部分用深度学习怼图片文本不一样，以前kaggle赛题，尤其是2019年前，有很\ **大比例是工业界的表格数据。比如各种实际的预测预估任务，CTR，信用评分，销量预测等**\ 。他们有如下几个特点。

**1.工业界的数据脏。异常点，缺失值，历史遗留问题造成的数据痕迹等等。**

**2.工业界的数据可解释性很强，每一列有真实的业务含义。**

**Q2.GBDT和NN有什么特点？**

**树模型天然的优点**

**1.天然的鲁棒性，能自动从异常点，缺失值学到信息。不需要归一化。直接上手一把梭。**

**2.树可以半自动化地完成一些特征非线性表达的工作，而且基于贪心切分+采样等抗过拟合手段，能比较好的挖掘数据的非线性。**

**3.树的可解释性很好，能生产特征重要性，帮助你理解数据，改善特征工程。**

**一个经典的套路是思考topN特征背后的逻辑，并围绕他们进行特征工程。**

**NN模型的优点**

**1.全自动化的特征工程和登峰造极的非线性表达能力，在数据表征范式统一，语义含义统一的稠密数据上（典型文本图像）上，NN一个打十个。另外，典型的像ID序列这种，人很难做出花来。也就是Bag
of words或者借用embedding表达一下，还有一半是NN的功劳。**

**2.NN模型容量极大，在数据量的加持上，放大了1的优势。**

+------+
|      |
+======+
| GBDT |
+------+
| NN   |
+------+

**Q3为什么不是SVM和LR？**

1.这两种模型获取非线性的方式太粗暴了，有种大炮打蚊子的感觉。\ **依靠kernel强行把VC维提高，带来的噪声特别多，有用信息很少，并且kernal是有先验的，很容易被人设的参数带跑偏。这在实际业务数据中是非常致命的。**

**2.理论上LR+完美的特征工程可以很强，但是太难了，又不是人人都是特征工程大师。**\ 早期凤巢亿级特征跑LR效果特别好逐渐成为传说。

**Q4.怎么根据数据特点进行模型选型？**

可以从这4方面来的。

**A.数据量大小**

**B.数据到预测目标的非线性**

**C.单列数据可解释性**

**D.特征工程天花板高低**

**XGB/LGB/CTB在最后两个上很有优势。**

**NN在前两个方面很有优势。**
